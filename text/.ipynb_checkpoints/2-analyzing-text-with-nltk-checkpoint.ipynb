{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Text With NLTK\n",
    "\n",
    "## Overview\n",
    "Basic text analytics with NLTK\n",
    "\n",
    "## Runtime\n",
    "20 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Wordcount on NLTK Corpus\n",
    "Let's do some basic word counts in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ebc3a40d3d20>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ebc3a40d3d20>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    print (\"gw2006_words : \\n\", ???)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import state_union\n",
    "from nltk import FreqDist\n",
    "\n",
    "## -- state of the union\n",
    "print(state_union.fileids())\n",
    "print(\"---\")\n",
    "\n",
    "gw2006 = state_union.raw('2006-GWBush.txt')\n",
    "print (\"len of gw2006 : \" , len(gw2006))\n",
    "print (\"gw2006 :\\n\",  gw2006[1:300])\n",
    "print(\"---\")\n",
    "\n",
    "## TODO-2 :  get the words\n",
    "gw2006_words = state_union.words('2006-GWBush.txt')\n",
    "## TODO-3 : print number of words and some words (Hint : gw2006_words[1:100])\n",
    "print(\"len(gw2006_words) : \", len(gw2006_words[1:100]))\n",
    "print (\"gw2006_words : \\n\", ???)\n",
    "print(\"---\")\n",
    "\n",
    "fdist = nltk.FreqDist(gw2006_words)\n",
    "print (\"most common words : \" , fdist.most_common(50))\n",
    "## What do we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Cleaning up text\n",
    "In the previous example, our top words were 'the', 'and' , 'of'.  These are called 'stop words'.  Let's clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "gw2006_words = state_union.words('2006-GWBush.txt')\n",
    "gw2006_words_lower = [i.lower() for i in gw2006_words]\n",
    "print(\"len(gw2006_words) : \", len(gw2006_words))\n",
    "print(\"---\")\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "print (\"len(stop_words_en) : \", len(stop_words_en))\n",
    "print(\"stop words_en : \\n\", sorted(stop_words_en))\n",
    "print(\"---\")\n",
    "\n",
    "cleaned = [i for i in gw2006_words_lower if i not in stop_words_en]\n",
    "## TODO : how many words in cleaned\n",
    "print(\"len(cleaned) : \", ???)\n",
    "print(\"---\")\n",
    "\n",
    "print(\"cleaned text:\\n\", cleaned[1:200])\n",
    "print(\"---\")\n",
    "\n",
    "fdist = nltk.FreqDist(cleaned)\n",
    "print (\"most common words in cleaned :\\n\" , fdist.most_common(50))\n",
    "print(\"---\")\n",
    "\n",
    "# we need to remove punctuation\n",
    "stop_words_en.update(['-', '.', ',', '#', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "\n",
    "cleaned2 = [i for i in gw2006_words_lower if i not in stop_words_en]\n",
    "## TODO : how many words in cleaned\n",
    "print(\"len(cleaned2) : \", ???)\n",
    "print(\"---\")\n",
    "\n",
    "## TODO - calculate FreqDist for cleaned2\n",
    "fdist = nltk.FreqDist(???)\n",
    "print (\"most common words in cleaned2 :\\n\" , fdist.most_common(50))\n",
    "print(\"---\")\n",
    "\n",
    "## TODO :  further cleanup\n",
    "## Inspecting the output, we see a couple of punctuations that need cleaning up\n",
    "## add them to stop words list and clean up again\n",
    "stop_words_en.update(['.)', '??']) \n",
    "cleaned3 = [i for i in gw2006_words_lower if i not in stop_words_en]\n",
    "## TODO : len of cleaned3\n",
    "print(\"len(cleaned3) : \", ???)\n",
    "print(\"---\")\n",
    "\n",
    "## TODO : Commpute FreqDist for cleaned3\n",
    "fdist = nltk.FreqDist(???)\n",
    "print (\"most common words in cleaned3 :\\n\" , fdist.most_common(50))\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Analyze another text dataset\n",
    "In the previous exercise we analyzed data bundled with NLTK.\n",
    "In this section, we are going to load and analyze custom dataset\n",
    "\n",
    "We will use [State of the union 2014 by President Obama](../data/text/sotu-2014-obama.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "f = open('../data/text/sotu-2014-obama.txt')\n",
    "text = f.read()\n",
    "print(\"len(text)\", len(text))\n",
    "print(\"text:\\n\", text[1:500])\n",
    "print(\"---\")\n",
    "\n",
    "# Tokenize, split into words\n",
    "words = word_tokenize(text)\n",
    "words_lower = [i.lower() for i in words]\n",
    "print(\"len(words) : \", ???)\n",
    "print(\"words:\\n\", words[:50])\n",
    "print (\"---\")\n",
    "print(\"words_lower:\\n\", words_lower[:50])\n",
    "print (\"---\")\n",
    "\n",
    "# fdist\n",
    "fdist = nltk.FreqDist(words)\n",
    "print (\"most common in words :\\n\" , fdist.most_common(50))\n",
    "print (\"---\")\n",
    "\n",
    "\n",
    "## TODO : now use the above example to eliminate stop words from text\n",
    "## and run distribution\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "cleaned1 = [i for i in words_lower if i not in stop_words_en]\n",
    "print(\"len(cleaned1) : \", len(cleaned))\n",
    "print(\"---\")\n",
    "\n",
    "fdist = nltk.FreqDist(???)\n",
    "print (\"most common in cleaned1 :\\n\" , fdist.most_common(50))\n",
    "print (\"---\")\n",
    "\n",
    "## TODO : continue cleaning up further\n",
    "## remove punctuation\n",
    "## remove any other \n",
    "\n",
    "## Final output is \n",
    "## [('applause', 97), ('america', 39), ('help', 32), ('cheers', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Ngrams / Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
    "My dog likes to run too; so he had a great time.  \n",
    "I bought ice cream from the ice cream truck. Yummy!\n",
    "It was a perfect sunny day!\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "words_lower = [i.lower() for i in words]\n",
    "\n",
    "print (\"raw text (\", len(text) , \") : \\n\", text)\n",
    "print (\"---\")\n",
    "print (\"words (\" , len(words), \") : \\n\",words)\n",
    "print (\"---\")\n",
    "\n",
    "bigrams = nltk.ngrams(words, 2)\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "print(\"bigrams in raw text :\\n\", fdist.most_common())\n",
    "print (\"---\")\n",
    "\n",
    "\n",
    "## TODO : Now cleanup STOP words and calculate bigrams again\n",
    "\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_english.update(['-', '.', ',', '#', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "\n",
    "cleaned = [i for i in words_lower if i not in stop_words_english]\n",
    "print (\"cleaned words (\", len(cleaned), \"):\\n\", cleaned)\n",
    "print (\"---\")\n",
    "\n",
    "## Complete the following\n",
    "bigrams = nltk.ngrams(???, ???)\n",
    "fdist = nltk.FreqDist(???)\n",
    "print(\"bigrams in cleaned text:\\n\", fdist.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Stemming Part 1\n",
    "Let's explore different stemming algorithm available in NLTK.  Run the code below and observe the differences in stemming (marked by \\*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word,  snowball_stem,   porter_stem, lancaster stem\n",
      "run,  run,  run,  run\n",
      "running,  run,  run,  run\n",
      "* like,  like,  like,  lik\n",
      "* liked,  like,  like,  lik\n",
      "snow,  snow,  snow,  snow\n",
      "snowing,  snow,  snow,  snow\n",
      "dog,  dog,  dog,  dog\n",
      "dogs,  dog,  dog,  dog\n",
      "* maximum,  maximum,  maximum,  maxim\n",
      "* multiply,  multipli,  multipli,  multiply\n",
      "* crying,  cri,  cri,  cry\n",
      "leaves,  leav,  leav,  leav\n",
      "* fairly,  fair,  fairli,  fair\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "words = ['run', 'running', 'like', 'liked', 'snow', 'snowing', 'dog', 'dogs', 'maximum', \n",
    "         'multiply', 'crying', 'leaves', 'fairly']\n",
    "#print(words)\n",
    "stemmer_snowball = SnowballStemmer(\"english\")\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "#stemmed_tokens =  [stemmer.stem(word) for word in words]\n",
    "#print(stemmed_tokens)\n",
    "\n",
    "# run through a few stems\n",
    "print(\"word,  snowball_stem,   porter_stem, lancaster stem\")\n",
    "for w in words:\n",
    "    snowball_stem = stemmer_snowball.stem(w)\n",
    "    porter_stem = stemmer_porter.stem(w)\n",
    "    lancaster_stem = stemmer_lancaster.stem(w)\n",
    "    if (snowball_stem != porter_stem) or (snowball_stem != lancaster_stem) or (porter_stem != lancaster_stem):\n",
    "        print(\"* \", end='')\n",
    "    print (\"{},  {},  {},  {}\".format(w, snowball_stem, porter_stem, lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Putting it all together\n",
    "Now that we know all the algorithms, let's do a final analysis on 2014 SOTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os.path import expanduser\n",
    "nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "f = open('../data/text/sotu-2014-obama.txt')\n",
    "text = f.read()\n",
    "words = word_tokenize(text)\n",
    "\n",
    "## TODO : Cleanup 1 : lowercase it all\n",
    "words_lower = [i.???() for i in words]\n",
    "\n",
    "\n",
    "## cleanup 2 - remove stop words\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_english.update(['-', '.', ',', '#', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stop_words_english.update([\"'s\", '--'])\n",
    "## TODO : iterate through 'words_lower'\n",
    "cleaned1 = [i for i in ??? if i not in stop_words_english]\n",
    "print(\"len(cleaned1) : \", len(cleaned1))\n",
    "print (\"cleaned1:\\n\", cleaned1[:50])\n",
    "print(\"---\")\n",
    "\n",
    "##  Cleanup 3 - stem\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "## TODO : iterate through 'cleaned1'\n",
    "cleaned2 = [stemmer.stem(word) for word in ???]\n",
    "print (\"len(cleaned2) : \", len(cleaned2))\n",
    "print (\"cleaned2:\\n\", cleaned2[:50])\n",
    "print(\"---\")\n",
    "\n",
    "## TODO : Wordcount on 'cleaned2'\n",
    "wc = FreqDist(???)\n",
    "print(\"top 20 word count(cleaned2) : \" )\n",
    "for word, frequency in wc.most_common(20):\n",
    "    print(f\"{word} = {frequency}\")\n",
    "print(\"---\")\n",
    "    \n",
    "## bigrams\n",
    "## TODO : apply ngrams to 'cleaned2'\n",
    "bigrams = nltk.ngrams(???, 2)\n",
    "bigrams_fdist = FreqDist(bigrams)\n",
    "print(\"top 20 bigrams(cleaned2) : \" )\n",
    "for word, frequency in bigrams_fdist.most_common(20):\n",
    "    print(f\"{word} = {frequency}\")\n",
    "print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
